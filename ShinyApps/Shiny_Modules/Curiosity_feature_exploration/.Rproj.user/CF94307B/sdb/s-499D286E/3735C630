{
    "contents" : "Title\n========================================================\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).\n\nWhen you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n**Objective**:\n\nFramework:\n\nA) eq: input -> model -> output\n\nB) Reduce it to the lowest common unit\n\nC) Ask why? Why is this important?\n\nD) Generalization is a key concept. Random sampling....fewer + broad rules vs many + niche rules\n\n1. Context.Domain: Walmart sales. (Retail, lower income group, Arkansas hq, weekly sales revenue)\n\n1. Work backwards. What is the output?\n\n2. Output: 9 months of weekly sales (revenue, not inventory, assume price is consistent). Granularity: week\n\n3. Model:\n\n3a.  What are the simplifying equations: What is it made of? What would drive it? Think of the who? Who is the user? Who is the decision maker? In this case, the user is the grocery shopper...\n\n3b.  Week seasonality: Pay periods vs non pay periods (bi-weekly, pay check to pay check). Food stamps.\n\n3c.  Month seasonality: Based on seasons, winter vs summer (some products are weather sensitive)\n\n3d.  Holiday seasonaity: Type of holiday( xmas vs superbowl), holiday occuring on weekday vs a weekend\n\n3e.  External factors: Promo effects\n\n3f.  Trends: Grow customer base ( more shops, more customer base - from unemployed, other retailers). Grow engagement ( more spend from current customers - promos) \n\n\n**Exploration/Hypothesis Generation**:\n\n1. Hypothesis: Seasonality + Trend = Sales\n\na) Seasonality is similar year on year as seen on the charts\n\nb) Trend: 2011 ( 2nd half) -> 2010 + x , 2012 ( first half) -> 2010 + X. Is trend inc/dec/stop? What drives trend?\n\n\n```{r}\nsummary(cars)\n```\n\nYou can also embed plots, for example:\n\n```{r fig.width=7, fig.height=6}\nplot(cars)\n```\n\nlike the idea of calling it a sample, meaning that it's not the full population so we want to intepret abt the population\n```{r}\nsample<-read.csv('C:/Users/dwoo57/Google Drive/Career/Data Mining Competitions/Kaggle/Walmart - Inventory and weather prediction/Experiments/Gamma/Exp_A/Dept92_All_stores.csv')\nsample$Date<-as.Date(sample$Date,\"%m/%d/%Y\")\nstr(sample)\n\nsample2<-read.csv('C:/Users/dwoo57/Google Drive/Career/Data Mining Competitions/Kaggle/Walmart - Inventory and weather prediction/Experiments/Gamma/Exp_A/Dept92_All_stores_V2.csv')\nsample2$Date<-as.Date(sample2$Date,\"%m/%d/%Y\")\nstr(sample2)\n```\n\nknit may be abit slow for modeling etc. Good to have for documentation but may want to keep both codes open\nBased on the below\n```{r}\nlibrary(plyr)\nlibrary(lattice)  #xyplot\nlibrary(latticeExtra)  #layer_, panel.xblocks\nlibrary(gridExtra)  #grid.arrange\nlibrary(RColorBrewer)  #brewer.pal\nlibrary(ggplot2)\n\nsample.new<-ddply(sample, .(Date,Weeknum,Year,IsHoliday),summarise,Weekly_Sales = sum(Weekly_Sales))\nstr(sample.new)\nddply(sample, .(Year),summarise,Weekly_Sales = sum(Weekly_Sales))\n\nsample2.new<-ddply(sample2, .(Date,Weeknum_mod,Year,IsHoliday),summarise,Weekly_Sales = sum(Weekly_Sales))\nstr(sample.new)\nddply(sample2, .(Year),summarise,Weekly_Sales = sum(Weekly_Sales))\n```\n\n\n```{r fig.width=20, fig.height=6}\nggplot(sample.new, aes(Weeknum,Weekly_Sales)) + \n  geom_line( aes(colour = factor(Year) ),size = 1)  + \n  geom_point( aes(color = factor(IsHoliday)),size = 3.5)\n```\n\n\nTransformation:\n\n1. Aligning the weeks for comparison. Why it matter? Seasonality is consistent year on year, even with promos.\n\n2. So looks like next area is understanding the split. in 2011, 2nd half when it started splitting. \n\n2a.  Consider using average - want to generalize across stores. Also, when looking at stores can we group them. High level then low level\n\n```{r fig.width=20, fig.height=6}\nggplot(sample2.new, aes(Weeknum_mod,Weekly_Sales)) + \n  geom_line( aes(colour = factor(Year) ),size = 1)  + \n  geom_point( aes(color = factor(IsHoliday)),size = 3.5)\n```\n**Why this matters? This shows seasonality is similar year on year and trends are driving the delta yoy**\n\n\n**(NEXT) H1B: Trend -> Are yoy trend decreasing/increasing/constant/stop? What are drivers of trend?**\n\n2010 Seasonality + 2011 trend + 2011 values -> 2012\n2010 Seasonality can compute\n2011 trend is this increasing or decreasing?\n\nHow to measure trend?\nAssume seasonality is constant year on year and is additive.\n\nTake year on year delta:\n2010 to 2011\n2011 to 2012\n\nWhat do we need?\nAggregate by year\ncalculate the average weekly sales, maybe take the median\nthen take the year on year difference\n\n```{r fig.width=20, fig.height=6}\nlibrary(dplyr)\nlibrary(zoo)\n\nds<- sample2 %>% group_by(Year,Weeknum_mod) %>% summarize(avg = mean(Weekly_Sales))\n#then \nds1<-group_by(ds,Weeknum_mod)\n#this works because of the group by. It's smart to know how to do the difference\nds2<-mutate(ds1,diff_yoy = avg - lag(avg), per_diff_yoy = avg/lag(avg)-1)\nds3<-group_by(ds2,Year)\nds3<-mutate(ds3,mov_avg = rollapply(data = per_diff_yoy, width = 4, FUN = mean, align =\"right\",fill = NA ))\n\nggplot(ds3, aes(Weeknum_mod)) + \ngeom_line(aes(y=per_diff_yoy,color=factor(Year)),size = 0.5)  +\ngeom_line(aes(y=mov_avg,color=factor(Year)),size = 1)  \n```\n**Why this matters? Observation. A) Cross-over point b) Levelling or decreasing. The cross over seems to lead that the trend may be decreasing or normalizing**\n\n#Next steps\na) Why that cross over point?\nb) Also can we correlate any other features with this? Since we are not sure what is driving it\nc) Can we find the drivers that cause the trends?\nd) Correlate it with other features\ne) Maybe take 2011 and then especially when that cross over point happen or start to increase, any other variables that correlate with it\n\n\nlibrary(dplyr)\nlibrary(zoo)\n\nstr(sample2)\n\nds<- sample2 %>% group_by(Year,Weeknum_mod) %>% summarize(per_diff = mean(Weekly_Sales))\n\n#going to do this in steps\n# first find the average by year per week across stores\nds<- sample2 %>% group_by(Year,Weeknum_mod) %>% summarize(avg = mean(Weekly_Sales))\n#then \nds1<-group_by(ds,Weeknum_mod)\n#this works because of the group by. It's smart to know how to do the difference\nds2<-mutate(ds1,diff_yoy = avg - lag(avg), per_diff_yoy = avg/lag(avg)-1)\n#now add moving average, pick 4 so can see the monthly average\nds3<-group_by(ds2,Year)\nds3<-mutate(ds3,mov_avg = rollapply(data = per_diff_yoy, width = 4, FUN = mean, align =\"right\",fill = NA ))\n\nds3\nds2\n\n\n\n\n\n\nds2\nds1\nds\n\nDo we need to know what is causing X? Or more so is Trend up/down/same/zero? I think the second questions is more important\n\nThoughts:\n\n1. In 2011, determine when the split ocurred - what are the external drivers? What causes this\n2. Do we apply the same trend in 2012 for the first half?\n3. Can we generalize across stores? High level grouping?\n\nNext steps:\nQ: Since trends are all increasing, what are explanatory variables that can explain it? i.e unemployment rate?\n\n1. For store type A, take a random sample and plot difference year on year\n\na) Steps calculate year on year difference for the same store (done)\nb) now take random sample of stores\n\nstr(sample2)\n\n#First get a single store\nsample2.store39<-subset(sample2, Store == 39)\nstr(sample2.store39)\n\n#next how does the data look like\nhead(sample2.store39)\n# need to match where year = year + 1 and weeknum = weeknum\ninstall.packages(\"dplyr\")\nlibrary(dplyr)\n\nsample.tbl<-tbl_df(sample2.store39)\n\nhead(sample.tbl)\nstr(sample.tbl)\nsummary(sample.tbl)\n\nsample.tbl %>\nrequire(plyr)\nlibrary(quantmod)\n\n\nsubset.store39<-subset(sample2.store39, select =c('Date', 'Weekly_Sales','Weeknum_mod','Year'))\nstr(subset.store39)\nds_test$Store\n#diff uses the consective arguments\n\n#ddply(subset.store39, .(Weeknum_mod) , mutate, yoy = c(NA,diff(Weekly_Sales)/Weekly_Sales ))\n\nds<-ddply(subset.store39, .(Weeknum_mod) , transform, yoy_per = Delt(Weekly_Sales), yoy = c(NA,diff(Weekly_Sales)) )\n\nds<-subset(ds, Year ==2011)\n\nplot(ds$Weeknum_mod,ds$Delt.1.arithmetic, type ='l')\n\n\nb) Second part random sample\n\nset.seed(1234)\ntake <- sample(unique(sample2$Store), 10)\n\nNROW(unique(sample2$Store))\nncol(unique(sample2$Store))\n\ntake\n\nds_test<-sample2[sample2$Store %in% take, ]\n\nsummary(ds_test)\nds_test$Store<-as.factor(ds_test$Store)\nNROW(unique(ds_test$Store))\n\nc) now calculate diff \n\nds<-ddply(ds_test, .(Store,Weeknum_mod) , transform, yoy_per = Delt(Weekly_Sales), yoy = c(NA,diff(Weekly_Sales)) )\nds<-subset(ds, Year ==2011)\n\nstr(ds)\n\n#initial plot\n#what is out goal here...take a sample to see what is going on.\n#what are the questions we are interested? If they are all similar then should be similar to the average?\n#do we then just say that type A has this trend?\n#oh we wanted to understand what the drivers where, how they correlate to explanatory variables?\n\nlibrary(ggplot2)\nggplot(ds, aes(Weeknum_mod,Delt.1.arithmetic)) + \n  geom_line( aes(colour = factor(Store) ),size = 1)  + \n  geom_point(size = 2)\n\n#These are the next steps\n#very hard to see\n#unless we smooth it out to see what is going on\n#what is the monthly average\n\n\n\n",
    "created" : 1436553824360.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "638115873",
    "id" : "3735C630",
    "lastKnownWriteTime" : 1436390191,
    "path" : "C:/Users/dwoo57/Google Drive/Knowledge Base/R Scripts/ShinyApps/Shiny_Modules/Curiosity_feature_exploration/time_series/time_series_exploration_lab_notebook.Rmd",
    "project_path" : "time_series/time_series_exploration_lab_notebook.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}