{
    "contents" : "Title\n========================================================\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).\n\nWhen you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n**Objective**:\n\nFramework:\ninput -> model -> output\n1. Context.Domain: Walmart sales. (Retail, lower income group, Arkansas hq, weekly sales revenue)\n\n1. Work backwards. What is the output?\n\n2. Output: 9 months of weekly sales (revenue, not inventory, assume price is consistent). Granularity: week\n\n3. Model:\n\n3a.  What are the simplifying equations: What is it made of? What would drive it? Think of the who? Who is the user? Who is the decision maker? In this case, the user is the grocery shopper...\n\n3b.  Week seasonality: Pay periods vs non pay periods (bi-weekly, pay check to pay check). Food stamps.\n\n3c.  Month seasonality: Based on seasons, winter vs summer (some products are weather sensitive)\n\n3d.  Holiday seasonaity: Type of holiday( xmas vs superbowl), holiday occuring on weekday vs a weekend\n\n3e.  External factors: Promo effects\n\n3f.  Trends: Grow customer base ( more shops, more customer base - from unemployed, other retailers). Grow engagement ( more spend from current customers - promos) \n\n\n\n\n\nPrediction:\n\n1.Dependent variable : Weekly Sales (Assuming in dollars)>>> Continuous (vs Categorical) >> Regression\n\n2 How far in the future: Given 2.5 years predict 9 months from 2012 Nov to 2013 July\n\n3. Overall goal: 9 months into the future. This seems like a long time.\n\n4. Given 2.5 years of data....what is the smallest unit that we can predict here...say given what we have could we predict the last month? If we can then that would give us some confidence of the next month.\n\n5. Reduce it to the lowest common unit\n\n3.Equation: Prior priors -> 1 to 9 Months ahead\n\n4.Granularity: Across diff stores, across diff dept at the weekly level\n\nTraining/Testing:\n1. Training - we are given 2.5 years of history. Starting with 2010 - 2012. 2012 is when the markdowns were impl.\n2. This means baseline of 2010-2011 = baseline and then 2012 = markdowns effect\n3. Holidays are given more points\n4. Training can we partition 2012 into two parts or 4 holidays we split 2 - 2. Can we generalize across holidays are do we have to see the effect for each to extrapolate to the following year? Possibly each markdown would have a diff impact on holiday.\n5. Maybe what we can do is markdown impact on non-holiday vs holiday\n6. Typically recency is a good indicator\n7. trick is the following:\n7a: 2011 predicts 2011\n\n\n```{r}\nsummary(cars)\n```\n\nYou can also embed plots, for example:\n\n```{r fig.width=7, fig.height=6}\nplot(cars)\n```\n\nlike the idea of calling it a sample, meaning that it's not the full population so we want to intepret abt the population\n```{r}\nsample<-read.csv('C:/Users/dwoo57/Google Drive/Career/Data Mining Competitions/Kaggle/Walmart - Inventory and weather prediction/Experiments/Gamma/Exp_A/Dept92_All_stores.csv')\nsample$Date<-as.Date(sample$Date,\"%m/%d/%Y\")\nstr(sample)\n\nsample2<-read.csv('C:/Users/dwoo57/Google Drive/Career/Data Mining Competitions/Kaggle/Walmart - Inventory and weather prediction/Experiments/Gamma/Exp_A/Dept92_All_stores_V2.csv')\nsample2$Date<-as.Date(sample2$Date,\"%m/%d/%Y\")\nstr(sample2)\n```\n\nknit may be abit slow for modeling etc. Good to have for documentation but may want to keep both codes open\nBased on the below\n```{r}\nlibrary(plyr)\nlibrary(lattice)  #xyplot\nlibrary(latticeExtra)  #layer_, panel.xblocks\nlibrary(gridExtra)  #grid.arrange\nlibrary(RColorBrewer)  #brewer.pal\nlibrary(ggplot2)\n\nsample.new<-ddply(sample, .(Date,Weeknum,Year,IsHoliday),summarise,Weekly_Sales = sum(Weekly_Sales))\nstr(sample.new)\nddply(sample, .(Year),summarise,Weekly_Sales = sum(Weekly_Sales))\n\nsample2.new<-ddply(sample2, .(Date,Weeknum_mod,Year,IsHoliday),summarise,Weekly_Sales = sum(Weekly_Sales))\nstr(sample.new)\nddply(sample2, .(Year),summarise,Weekly_Sales = sum(Weekly_Sales))\n```\n\n From this figure, say we were to build a model for each year\n 2010 = seasonality\n 2011 = 2010_seasonality + 2011_Trend\n 2012 = 2010_seasonality + 2011_Trend + 2012_Trend + Markdowns\n 2013 = 2010_seasonality + 2011_Trend + 2012_Trend + 2013_Trend + Markdowns\n\nFrom 2010 and 2011 we can possibly determine the seasonality and trend\nWe possibly can assume seasonality reminds the same\nDo we assume the trend would repeat year on year? Thought earlier was whether there were explanatory variables that would explain the trend\nThen once we know this we can then determine the impact of the markdowns\n\n sounds like a reasonable place to start\n train data is all\n test data is the submission data\n eventually want to have cross-validation set. Maybe partition 2012. Take some weeks out and see what happens, especially non holiday weeks \n```{r fig.width=20, fig.height=6}\nggplot(sample.new, aes(Weeknum,Weekly_Sales)) + \n  geom_line( aes(colour = factor(Year) ),size = 1)  + \n  geom_point( aes(color = factor(IsHoliday)),size = 3.5)\n```\n\nTransformation:\n\n1. Aligning the weeks for comparison. Why it matter? Seasonality is consistent year on year, even with promos.\n\n2. So looks like next area is understanding the split. in 2011, 2nd half when it started splitting. \n\n2a.  Consider using average - want to generalize across stores. Also, when looking at stores can we group them. High level then low level\n\n```{r fig.width=20, fig.height=6}\nggplot(sample2.new, aes(Weeknum_mod,Weekly_Sales)) + \n  geom_line( aes(colour = factor(Year) ),size = 1)  + \n  geom_point( aes(color = factor(IsHoliday)),size = 3.5)\n```\nstrategy\n1.determine seasonality component\n2.Trend component. \n3.Goal for train set is to predict 2012 and then the submission file is the test set.\n\nHow does STL predict the trend?\nProbably what i would do is\n use 2010 to predict 2011 once i have this stable then predict 2012. then use this has a baseline\n if i use 2010 to predict 2011. Would i over predict?\n \n\n\n```{r fig.width=20, fig.height=6}\nlibrary(forecast)\nsample.new.subset<-subset(sample.new, Date >= \"2010-01-01\" & Date <= \"2012-02-28\")\ntrain_ts<- ts(sample.new.subset$Weekly_Sales, frequency=52, start = c(2010, 2,5))\nfit1 <- stl(train_ts,  s.window=\"periodic\", t.window = 52)\nplot(fit1)\nsummary(fit1)\naccuracy(forecast(fit1))\n```\n\nMAPE of 1.3% that seems low. Could potentially be overfitting.\nSome take aways:\nLooking at 2011. Also for accuracy what sample does it use? Uses in sample so all.\nIf look at the trend see that this is almost a linear curve, does this make sense? Why would consumption increase linearly over time?\nHow about remainder? Some assumptions, error should be random, meaning no correlation between points.\nMaybe can include some diagnostic plots here.\nSeems random\nNot increasing over time\n\nMaybe seasonality is really multiplicative, seems odd the trend would increase linearly over time. Should be step wise\nWhen to use additive vs multiplicative. Mainly does the seasonal component vary with time or volume? \nI think mainly of seasonal component is constant or does it vary with the 'volume' or varies with the actual trend.\nIs it a trend * seasonality or trend + seasonality. if varies with the trend that is multiplicative\nMaybe a framework is thinking of it in mathematical formulas there are only a few permulations\nx +-*/ y then (y * y ...) + z Only these variations\n\nSo now, we have framework for decomposing into seasonality + trend\nWe have a notebook that would help us rehash and formulate the analysis\nNext, we are looking into the predictability or how generalizable is this model.\n\nGeneralizability:\nSeasonality i think make sense\nJust the trend is odd. Why would it increase linearly over time.\nUnless population grows that rapidly?\n\nCan we check unemployment rate?\n\n**Transformations that helped**:\n1. Offseting the time series helps - aligning the weeks\n\n```{r fig.width=20, fig.height=6}\nlibrary(forecast)\nsample.new.subset<-subset(sample.new, Date >= \"2010-01-01\" & Date <= \"2012-02-28\")\ntrain_ts<- ts(sample.new.subset$Weekly_Sales, frequency=52, start = c(2010, 2,5))\nfit1 <- stl(train_ts,  s.window=\"periodic\", t.window = 52)\nplot(fit1)\nsummary(fit1)\naccuracy(forecast(fit1))\n```\n\n\nPotential resources:\nComputing baseline sales\nhttp://www.tabsgroup.com/wp-content/uploads/2012/12/modeltoimprovetheestimationofbaselineretailsales.pdf\n",
    "created" : 1435779631830.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2067676511",
    "id" : "CB278793",
    "lastKnownWriteTime" : 1436216241,
    "path" : "C:/Users/dwoo57/Google Drive/Knowledge Base/R Scripts/ShinyApps/Shiny_Modules/Curiosity_feature_exploration/time_series/time_series_exploration_lab_notebook.Rmd",
    "project_path" : "time_series/time_series_exploration_lab_notebook.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}